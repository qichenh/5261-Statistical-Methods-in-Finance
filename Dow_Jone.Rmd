---
title: "STAT 5261 Project"
author: "Yuqiao Wang, Yuying Zhou, Qichen He, Chenxi Wang, Shoujing Fu, Keyu Liu"
date: "April 26, 2020"
output: html_document 
---
# Dow Jones Industrial Average Analysis 

```{r setup, include=FALSE}
library(forecast)
library(tseries)
library(rugarch)
library(FinTS)
```


```{r}
#Plot series
data <-  read.csv('^DJI(4).csv')
Dow_Jones <- ts(data$Adj.Close, start=c(2005,1), end=c(2019,12),frequency = 251)
x <- Dow_Jones
ts.plot(Dow_Jones,main='Dow Jones Industrial Average')

#Plot trend
trend_dj<- ma(Dow_Jones, order = 12, centre = T)
plot(as.ts(Dow_Jones), ylab = "trend")

##Detrend the Time Series / Plot seasonality
detrend_dj<- Dow_Jones /trend_dj
plot(as.ts(detrend_dj))

```


```{r}
##Average the Seasonality
m_dj<-t(matrix(data = detrend_dj, nrow = 251))
seasonal_dj= colMeans(m_dj, na.rm = T)
plot(as.ts(rep(seasonal_dj,251)))

##Random noise
random_dj = Dow_Jones/ (trend_dj * seasonal_dj)
plot(as.ts(random_dj))

##decompose time series
decomposedRes <- decompose(Dow_Jones, type="mult") 
plot(decomposedRes)
```


```{r}
##Test stationary
price<-data$Adj.Close
log_returns <- diff(log(price), lag=1)

Box.test(log_returns, lag = 20, type ="Ljung")
# The Ljung-Box test statistic has an extremely small p-value < 2.2e-16
# so the null hypothesis of white noise is strongly rejected.
# At least one of the first 20 autocorrelation is nonzero.
# An AR(1) model to account for the small amount of autocorrelation might be appropriate.

adf.test(log_returns)
##For adf.test, the p-valus is 0.01 which is less than 0.05, we reject null hypothesis, 
##the time series is stationary.
pp.test(log_returns) #p-value=0.01
kpss.test(log_returns) #p-value=0.1
# The augmented Dickey-Fullertest, the Phillips-Perron tests and the KPSS test all suggest that
# the S&P 500 log return series is stationary.
```


```{r}
## acf and pacf
acf(log_returns)
pacf(log_returns)

# From the above plots, we deduce that an MA(2) model (where MA stands for moving average) 
# may fits our data because the ACF cuts off at one significant lag and the PACF shows geometric decay.
# As expected the first two peaks are significant. However, so is the third and fourth. 
# But we can legitimately suggest that this may be due to sampling bias as we expect 
# to see 5% of the peaks being significant beyond.

# Also, the PACF is useful for identifying the order of an AR process.
# A sign that a time series can be fit by an AR(P) is that the sample PACF will be nonzero up to p
# and the will be nearly zero for larger lags.
# From the above plots, we deduce that an AR(3) Model may fits our data because the PACF cuts off 
# at three signidicant lag.

```


## Use ARIMA to find the best arima model

```{r}
auto.arima(log_returns,ic="aic",seasonal=TRUE, trace=TRUE, allowdrift = FALSE)
fit1<-Arima(log_returns, order=c(3,0,2))
summary(fit1)
```

## check the model residuals

```{r}
par(mfrow=c(2,2))
acf(fit1$residuals)
pacf(fit1$residuals)
acf(fit1$residuals^(2))
pacf(fit1$residuals^(2))
```


## ETS Model
```{r}
# we fit an ETS model, and we will compare it with the previous ARIMA model
dow_ets <- ets(log_returns, model = "ZZZ")
dow_ets
```


## Cross Validation: ARIMA & ETS Model 

```{r}
# set up cross validation
n <- length(log_returns)

mse_arima <- c()
mse_ets <- c()
pred_all_arima <- c()
pred_all_ets <- c()

# Train each day except for the first 60 days
for (i in 60:(n-1)) {
  
  train <- log_returns[1:i]
  test <- log_returns[i+1]
  
  fit_arima <- Arima(train, order=c(3,0,2), method = "ML")  # ARIMA(3, 0, 2)
  pred_arima <- forecast(fit_arima, h = 1)
  
  fit_ets <- ets(train, model = "ANN") # ETS(A, N, N)
  pred_ets <- forecast(fit_ets, h = 1)
  
  mse_arima <- c(mse_arima, pred_arima[['mean']] - test)
  mse_ets <- c(mse_ets, pred_ets[['mean']] - test)
  pred_all_arima <- c(pred_all_arima, pred_arima[['mean']])
  pred_all_ets <- c(pred_all_ets, pred_ets[['mean']])
}

print(paste0("ARIMA MSE: ", mean(mse_arima^2))) # 0.000120
print(paste0("ETS MSE: ", mean(mse_ets^2))) # 0.000119

# plot of predicted data using ARIMA, ETS and plot of real data
plot(log_returns[61:n], type = "l", ylab = "DJI log return")
lines(pred_all_arima, col = "red")
legend("bottomright", legend = c("Original data", "ARIMA Prediction"), 
       col = c("black", "red"), lty = 1)

plot(pred_all_ets, type = "l", ylab = "ETS Prediction on DJI")

# we choose ARIMA(3,0,2) model
# Predict the stock returns of the next 79 days
log_returns %>% Arima(order=c(3,0,2), method = "ML") %>% 
  forecast(h = 79) %>% autoplot()

log_returns %>% ets(model = "ANN") %>% 
  forecast(h = 79) %>% autoplot()

# Actual returns of next 79 days
DJI_2020 <- read.csv("DJI 2020.csv")

log_return_2020 <- diff(log(DJI_2020$Adj.Close), lag = 1)

plot(log_return_2020, type = "l", col = "4", 
     ylab = "Stock Return", ylim = c(-0.13, 0.13),
     main = "DJ Return from Jan 2 - April 24, 2020")
```


## GARCH models
```{r}
# plot of the differenced data: we can observe many clusters of high volatility and low volatility
ts.plot(diff(log_returns))

# The ArchTest verifies that Arch effects exist
ArchTest(log_returns)


# We use Garch(1,1) model for the analysis of the volatility of stock prices 
dow_jones_spec <- ugarchspec(mean.model = list(armaOrder = c(3,2)),
                             variance.model=list(garchOrder=c(1,1)))

fit_dow <- ugarchfit(data=log_returns, spec=dow_jones_spec)
fit_dow

# acf, pacf, plots of residuals & squared residuals 
acf(fit_dow@fit$residuals)
pacf(fit_dow@fit$residuals)
acf(fit_dow@fit$residuals^(2))
pacf(fit_dow@fit$residuals^(2))


# forecast volatility of the next 10 days (two weeks)
garch_fc <- ugarchforecast(fit_dow, n.ahead = 10)
plot(garch_fc@forecast$sigma, ylab = "sigma", main = "Dow Jones")

```

