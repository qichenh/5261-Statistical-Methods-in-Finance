---
title: "STAT 5261 Project"
author: "Yuqiao Wang, Yuying Zhou, Qichen He, Chenxi Wang, Shoujing Fu, Keyu Liu"
date: "April 26, 2020"
output: html_document  
---
# S&P 500 Analysis 

```{r setup, include=FALSE}
library(forecast)
library(tseries)
library(FinTS)
library(rugarch)
```

##Plot time series
```{r}
##Plot time series
data <- read.csv('GSPY 15Y.csv')
sp_500 <- ts(data$Adj.Close, start=c(2005,1), end=c(2019,12),frequency = 251)
ts.plot(sp_500,main='S&P 500')


##Detect the Trend
trend_sp<- ma(sp_500, order = 12, centre = T)
plot(as.ts(sp_500))
lines(trend_sp)
plot(as.ts(trend_sp))


##Detrend the Time Series
detrend_sp<-sp_500 / trend_sp
plot(as.ts(detrend_sp))


##Average the Seasonality
m_sp<-t(matrix(data = detrend_sp, nrow = 251))
seasonal_sp= colMeans(m_sp, na.rm = T)
plot(as.ts(rep(seasonal_sp,251)))


##Random noise
random_sp = sp_500 / (trend_sp * seasonal_sp)
plot(as.ts(random_sp))


##decompose time series
ts_sp<- ts(sp_500, frequency = 251)
decompose_sp<- decompose(ts_sp, "multiplicative")
plot(as.ts(decompose_sp$seasonal))
plot(as.ts(decompose_sp$trend))
plot(as.ts(decompose_sp$random))
plot(decompose_sp)
```



##Test stationary
```{r}
##Test stationary
price<-data$Adj.Close
log_returns <- diff(log(price), lag=1)

Box.test(log_returns, lag = 20, type ="Ljung")
# The Ljung-Box test statistic has an extremely small p-value,
# so the null hypothesis of white noise is strongly rejected.
# At least one of the first 20 autocorrelation is nonzero.
# An AR(1) model to account for the small amount of autocorrelation might be appropriate.

adf.test(log_returns)
##For adf.test, the p-valus is 0.01 which is less than 0.05, we reject null hypothesis, 
##the time series is stationary.
pp.test(log_returns) #p-value=0.01
kpss.test(log_returns) #p-value=0.1
# The augmented Dickey-Fullertest, the Phillips-Perron tests and the KPSS test all suggest that
# the S&P 500 log return series is stationary.
```



## acf and pacf
```{r}
## acf and pacf
acf(log_returns)
pacf(log_returns)

# From the above plots, we deduce that an MA(1) model (where MA stands for moving average) may fits our data because the ACF cuts off at one significant lag and the PACF shows geometric decay.

# Also, the PACF is useful for identifying the order of an AR process.
# A sign that a time series can be fitted by an AR(P) is that the sample PACF will be nonzero up to p
# and the will be nearly zero for larger lags.
# From the above plots, we deduce that an AR(2) Model may fit our data because the PACF cuts off at two significant lags.
```



## Fit ARIMA Model
```{r}
## Fit ARIMA Model

# ARIMA Model is based on ARMA Model
# The model with the least AICc value is selected.

# According to the first section, the log return of S&P 500 is stationary.
# Use ARIMA to find the best arima model
ts.plot(log_returns,main='Daily Log Returns of S&P 500')

auto.arima(log_returns,ic="aic",seasonal=TRUE, trace=FALSE, allowdrift = FALSE)
fit1<-Arima(log_returns, order=c(2,0,1))
summary(fit1)

# check the model residuals
#plot(fit1$residuals)
acf(fit1$residuals)
pacf(fit1$residuals)
acf(fit1$residuals^(2))
pacf(fit1$residuals^(2))

# The residual plots look fine
# But the squared residuals not so good
```



## ETS Model
```{r}
# we fit an ETS model, and we will compare it with the previous ARIMA model
spy_ets <- ets(log_returns, model = "ZZZ")
spy_ets

```

## Cross Validation: ARIMA & ETS Model 
```{r}
# set up cross validation
n <- length(log_returns)

mse_arima <- c()
mse_ets <- c()
pred_all_arima <- c()
pred_all_ets <- c()

# Train each day except for the first 60 days
for (i in 60:(n-1)) {
  
  train <- log_returns[1:i]
  test <- log_returns[i+1]
  
  fit_arima <- Arima(train, order=c(2,0,1), method = "ML")  # ARIMA(2, 0, 1)
  pred_arima <- forecast(fit_arima, h = 1)
  
  fit_ets <- ets(train, model = "ANN") # ETS(A, N, N)
  pred_ets <- forecast(fit_ets, h = 1)
  
  mse_arima <- c(mse_arima, pred_arima[['mean']] - test)
  mse_ets <- c(mse_ets, pred_ets[['mean']] - test)
  pred_all_arima <- c(pred_all_arima, pred_arima[['mean']])
  pred_all_ets <- c(pred_all_ets, pred_ets[['mean']])
}

print(paste0("ARIMA MSE: ", mean(mse_arima^2))) # 0.000136
print(paste0("ETS MSE: ", mean(mse_ets^2))) # 0.000137

# plot of predicted data using ARIMA, ETS and plot of real data
plot(log_returns[61:n], type = "l", ylab = "SPY log return")
lines(pred_all_arima, col = "red")
legend("bottomright", legend = c("Original data", "ARIMA Prediction"), 
       col = c("black", "red"), lty = 1)

plot(pred_all_ets, type = "l", ylab = "ETS Prediction on SPY")

# we choose ARIMA(2,0,1) model
# Predict the stock returns of the next 79 days
log_returns %>% Arima(order=c(2,0,1), method = "ML") %>% 
  forecast(h = 79) %>% autoplot()

log_returns %>% ets(model = "ANN") %>% 
  forecast(h = 79) %>% autoplot()

# Actual returns of next 79 days
sp500_2020 <- read.csv("GSPC 2020.csv") 

log_return_2020 <- diff(log(sp500_2020$Adj.Close), lag = 1)

plot(log_return_2020, type = "l", col = "4", 
     ylab = "Stock Return", ylim = c(-0.13, 0.13),
     main = "S&P 500 Return from Jan 2 - April 24, 2020")

```


## GARCH models  
```{r}
# plot of the differenced data: we can observe many clusters of high volatility and low volatility
ts.plot(log_returns)

# The ArchTest verifies that Arch effects exist
ArchTest(log_returns)

# We use Garch(1,1) model for the analysis of the volatility of stock prices 
SPYspec <- ugarchspec(mean.model = list(armaOrder = c(2,1)),
                      variance.model=list(garchOrder=c(1,1)))
SPYspec
fit2 <- ugarchfit(data=log_returns, spec=SPYspec)
fit2


# acf, pacf, plots of residuals & squared residuals 
acf(fit2@fit$residuals)
pacf(fit2@fit$residuals)
acf(fit2@fit$residuals^(2))
pacf(fit2@fit$residuals^(2))


# forecast volatility of the next 10 days (two weeks)
garch_fc <- ugarchforecast(fit2, n.ahead = 10)
plot(garch_fc@forecast$sigma, ylab = "sigma", main = "S&P 500")



```



